{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Packages and Libraries ##\n",
    "\n",
    "# Web parcing, scraping, etc.\n",
    "import bs4 as bs # BeautifulSoup4 \n",
    "import urllib3\n",
    "import re\n",
    "import requests # HTTP parser\n",
    "import html5lib\n",
    "\n",
    "# DataFrames and math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Output related packages \n",
    "import pprint as pp\n",
    "import json\n",
    "\n",
    "# Progress bar and delaying requests \n",
    "from tqdm import tnrange, tqdm_notebook #progress bars\n",
    "from random import randint\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stretch Jupyter coding blocks to fit screen\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\")) \n",
    "\n",
    "# make it run on py2 and py3\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining I\n",
    "This  notebook is intended to perform the following processes:\n",
    "\n",
    "    1.1 Read-in news articles from newsAPI for a given date range, and up to five queries (passed as a list).\n",
    "\n",
    "    1.2 Extract features native to the articles (e.g. url).\n",
    "\n",
    "    1.3 Perform data cleanup and preprocessing.\n",
    "\n",
    "    1.4 Split dataset into n-csv-files for distrubuted computation or batching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Documentation__\n",
    "\n",
    "newsAPI (https://newsapi.org/) has limited documentation as to how their backend works. However, when developing this notebook the following website was frequently referenced -- as their documentation appears to closely match the behavior of newsAPI. \n",
    "\n",
    "https://docs.aylien.com/newsapi/#getting-started\n",
    "\n",
    "Moreover, here is the user agreement:\n",
    "\n",
    "https://newsapi.org/terms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### **Begin Data Mining I:** Read-in NewsAPI feed for a given date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'newsapi.newsapi_client.NewsApiClient'>\n"
     ]
    }
   ],
   "source": [
    "### NEWSAPI RELATED ###\n",
    "# keys: \n",
    "#rhkey = '847446b32283474fafd2aec7f95e502b'\n",
    "#r1key  = '1da951c142304f7bab52ba8e3970495b'\n",
    "#r2key = '40d53e49ee3543a3b162e6a453e2e373'\n",
    "#m1key = '211fc2107848473e99c1f235b400a07f'\n",
    "#m2key = 'c0f99eab932d4cabb61c23239f3f482d'\n",
    "m3key = '658cd65a714349fdbb7e8dd6ce59e9c4'\n",
    "#m4key  = '8ba091b7a47b4c9a9162a83ca72eb1ca'\n",
    "#e1key  = '2bc85776a0c14af6b9937366ad683e2f'\n",
    "#e2key = '22e5c3a8f0ee4fa59aaf384ba9395a86'\n",
    "#e3key = 'c554f8fb27ca4be1862192b44ee4425d'\n",
    "\n",
    "\n",
    "# Install API \n",
    "#!pip install newsapi-python\n",
    "\n",
    "# Import Client\n",
    "from newsapi import NewsApiClient\n",
    "\n",
    "# Initialize Client (create object)\n",
    "news_api = NewsApiClient(api_key = m3key)\n",
    "print(type(news_api))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.1 Read-in news articles from newsAPI for a given date range__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: **get_news**\n",
    "Function establishes values to be used for control of loop then calls functions used to extract news article data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(query, start, stop, sort, lang, article_count, page_count, init):\n",
    "    \"\"\"\n",
    "    control function for iterating over 100 pages of newsAPI's content \n",
    "    function then controls subordinate fuctions that extract ~100 articles per each page\n",
    "    \"\"\"\n",
    "    \n",
    "    import math\n",
    "    import time  \n",
    "    \n",
    "    # extract information about response file to ensure proper loop control\n",
    "    params = get_params(query, start, stop, sort, lang, article_count, page_count)\n",
    "\n",
    "    # variable referencing\n",
    "    status = params['status']\n",
    "    results = params['totalResults']\n",
    "\n",
    "    # Confirmation of data extraction\n",
    "    print(\"\\nVerify Read-in Process:\", status)\n",
    "    print(\"Number of Articles Correctly Read: \", results)\n",
    "    print(type(params), params.keys())\n",
    "           \n",
    "    # per page article extraction stop variable -- if number of articles is greater than number articles per page\n",
    "    loops = math.ceil(results/article_count)\n",
    "    \n",
    "    # batching control\n",
    "    begin = 0 + init\n",
    "    terminate = article_count + init\n",
    "    print(\"Total number of iterations (pages):\",loops)\n",
    "    \n",
    "    # check to ensure loop does not extract over 100pages*100endpoints=10000 articles\n",
    "    if loops < terminate:\n",
    "        terminate = loops\n",
    "        \n",
    "    print(\"Page range being extracted\", begin, terminate)\n",
    "    \n",
    "    if page_count == 'all' or article_count <  results:\n",
    "        print(\"\\n\\nExtracting News Data...\\n\")\n",
    "        full_df = pd.DataFrame()\n",
    "    \n",
    "        # function is called withinin while, is subject to number of pages available as a function of total no. articles\n",
    "        while begin < terminate:\n",
    "            begin += 1\n",
    "            page = begin  # for referencing clarity\n",
    "            \n",
    "            ### newsAPI has MAX HIT LIMIT of 60 per minute ### \n",
    "            time.sleep(1)   # delay of 1 second\n",
    "            print(\"Extracting Page\", page)\n",
    "\n",
    "            # call sequencial pages of articles and appends to df\n",
    "            df = news_data(query, start, stop, sort, lang, article_count, page)\n",
    "            full_df = full_df.append(df, ignore_index = True)\n",
    "            \n",
    "        print('Batch extraction completed:',begin,'of',terminate)\n",
    "        return(full_df)            \n",
    "    else:\n",
    "        # extracts articles assuming articles >= pages\n",
    "        print(\"Possible Invalid Parameters: Check values\")\n",
    "        brief_df = news_api.get_everything(q = query,\n",
    "                                          from_parameter= start,\n",
    "                                          to= stop,\n",
    "                                          sort_by= sort,\n",
    "                                          language= lang,\n",
    "                                          page_size= int(article_count)\n",
    "                                         )\n",
    "        return(brief_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: **get_params**\n",
    "Function runs an initial newsAPI call, used to store values for controlling loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(query, start, stop, sort, lang, article_count, page_count):\n",
    "    \"\"\"\n",
    "    function accepts similar parameters to master function get_news.\n",
    "    get_params is used to extract parameters to be used in controlling other functions \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nExtracting Parameters for newsAPI...\\n\")\n",
    "    params = news_api.get_everything(q = query,\n",
    "                                     from_parameter= start,\n",
    "                                     to= stop,\n",
    "                                     sort_by= sort,\n",
    "                                     language= lang,\n",
    "                                     page_size= int(article_count)\n",
    "                                    )\n",
    "    \n",
    "    # Confirmation of data extraction\n",
    "    print(\"Read-in Status of Given Date Range:\", params['status'])\n",
    "    print(\"Number of Articles in Given Date Range: \", params['totalResults'])\n",
    "    \n",
    "    return(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: **news_data**\n",
    "Function handles cases, and extracts values within 'articles'. Returns dataframe of contents: \n",
    "\n",
    "\n",
    "*Index(['author', 'description', 'publishedAt', 'source', 'title', 'url','urlToImage'],dtype='object')*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_data(query, start, stop, sort, lang, article_count, page):\n",
    "    \"\"\"\n",
    "    Principal data extraction function - can handle various relationships between no.pages and no.articles \n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(page, int):\n",
    "        params = news_api.get_everything(q = query,\n",
    "                                         from_parameter= start,\n",
    "                                         to= stop,\n",
    "                                         sort_by= sort,\n",
    "                                         language= lang,\n",
    "                                         page_size= int(article_count),\n",
    "                                         page = int(page)\n",
    "                                        )\n",
    "    ########### if params['articles'] throws error ########### \n",
    "    # either endpoint limit was met (10000)                  #\n",
    "    # too many endpoint requests in a month (1000 per month) #\n",
    "    # change to new api key.                                 #\n",
    "    ##########################################################\n",
    "    return(pd.DataFrame(params['articles'])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User provided parameters and function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#01/26/18 to 03/26/18\n",
    "query = 'Bitcoin'         # can handle a list of up to five search topics\n",
    "start = '2018-04-17'      # yyyy-mm-dd\n",
    "stop  = '2018-04-23'\n",
    "sort  = 'publishedAt'\n",
    "lang  = 'en'\n",
    "article_count = 100       # default is 20\n",
    "page_count = 'all'        # enter 1, 2, ... Notes: 'all' iterates over all articLes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "---\n",
    "#### __NOTE__\n",
    "\n",
    "Since newsAPI has a daily limit of endpoint requests (1000 per day), batching needs to be implemented.  The following cells controls the initialization for subsequent cells -- to ensure appropropriate, and sequential article extraction.\n",
    "\n",
    "Also, webAPI has a limit of 10,000 articles per key -- unless you want to pay for the monthly access.\n",
    "___\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------------------------------------------------------------------------- ##\n",
    "## KEY: Initialize accroding to batch number (i.e. n*100, where n is the batch) ##\n",
    "## USE: n > 0 ---- only once we can extract above 10000 articles\n",
    "n = 0\n",
    "init = n*100\n",
    "## ---------------------------------------------------------------------------- ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Parameters for newsAPI...\n",
      "\n",
      "Read-in Status of Given Date Range: ok\n",
      "Number of Articles in Given Date Range:  2496\n",
      "\n",
      "Verify Read-in Process: ok\n",
      "Number of Articles Correctly Read:  2496\n",
      "<class 'dict'> dict_keys(['status', 'totalResults', 'articles'])\n",
      "Total number of iterations (pages): 25\n",
      "Page range being extracted 0 25\n",
      "\n",
      "\n",
      "Extracting News Data...\n",
      "\n",
      "Extracting Page 1\n",
      "Extracting Page 2\n",
      "Extracting Page 3\n",
      "Extracting Page 4\n",
      "Extracting Page 5\n",
      "Extracting Page 6\n",
      "Extracting Page 7\n",
      "Extracting Page 8\n",
      "Extracting Page 9\n",
      "Extracting Page 10\n",
      "Extracting Page 11\n",
      "Extracting Page 12\n",
      "Extracting Page 13\n",
      "Extracting Page 14\n",
      "Extracting Page 15\n",
      "Extracting Page 16\n",
      "Extracting Page 17\n",
      "Extracting Page 18\n",
      "Extracting Page 19\n",
      "Extracting Page 20\n",
      "Extracting Page 21\n",
      "Extracting Page 22\n",
      "Extracting Page 23\n",
      "Extracting Page 24\n",
      "Extracting Page 25\n",
      "Batch extraction completed: 25 of 25\n"
     ]
    }
   ],
   "source": [
    "# object is the result of the following functions: 'get_params', 'get_news', and 'get_data'\n",
    "news = get_news(query, start, stop, sort, lang, article_count, page_count, init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore nested key/value pairs from newsAPI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author', 'description', 'publishedAt', 'source', 'title', 'url',\n",
       "       'urlToImage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ENSURE API WAS ACCESSED ##\n",
    "# if news.keys includes 'author', 'description', etc., success!\n",
    "\n",
    "news.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2496\n",
      "Index(['author', 'description', 'publishedAt', 'source', 'title', 'url',\n",
      "       'urlToImage'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2488"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(news))\n",
    "print(news.keys())\n",
    "news.head(5)\n",
    "len(news['url'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.2 Extract features native to the articles__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: **get_info**\n",
    "Function extracts variables from dataframe and stores each as a list, returning all of them as a single dataframe.\n",
    "\n",
    "__Note:__ *urlToImage* is not included in this process, as we are uncertain as to the value of the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_info(df):\n",
    "    \"\"\"\n",
    "    Accepts a dataframe of newsAPI articles, and controls all subbordinate functions that preprocess data\n",
    "    \"\"\"\n",
    "    \n",
    "    import copy\n",
    "    \n",
    "    author = []\n",
    "    title = []\n",
    "    publisher = []\n",
    "    publish_url = []\n",
    "    timeStamp = []\n",
    "    description = []\n",
    "    \n",
    "    # loop appends rows to respective lists \n",
    "    for col_name in df:\n",
    "        for index in df[col_name]:\n",
    "            if col_name == 'author':\n",
    "                author.append(index)\n",
    "            elif col_name == 'title':\n",
    "                title.append(index)\n",
    "            elif col_name == 'source':\n",
    "                name = index['name']\n",
    "                publisher.append(name)\n",
    "            elif col_name == 'url':\n",
    "                publish_url.append(index)\n",
    "            elif col_name == 'publishedAt':\n",
    "                timeStamp.append(index)\n",
    "            elif col_name == 'description':\n",
    "                description.append(index)\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    # merge lists and return them as dataframe.\n",
    "    df = pd.DataFrame({'author' : author,\n",
    "                       'title' : title,\n",
    "                       'publisher' : publisher,\n",
    "                       'source_url' : publish_url,\n",
    "                       'timeStamp' : timeStamp,\n",
    "                       'description' : description})\n",
    "    \n",
    "    return(df)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completed newsAPI Read-in Process: \n",
    "##### newsDF contains features extracted from raw newsAPI feed, for a given data range, and query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object creation\n",
    "newsDF = get_info(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Dimensions: (2496, 6) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>description</th>\n",
       "      <th>publisher</th>\n",
       "      <th>source_url</th>\n",
       "      <th>timeStamp</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pjbyrne</td>\n",
       "      <td>Disclaimer: English lawyer, not practising thi...</td>\n",
       "      <td>Prestonbyrne.com</td>\n",
       "      <td>https://prestonbyrne.com/2018/04/23/on-ethereu...</td>\n",
       "      <td>2018-04-23T23:59:35Z</td>\n",
       "      <td>Whether Ethereum is a security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tyler Durden</td>\n",
       "      <td>Authored by Caitlin Johnstone via Medium.com, ...</td>\n",
       "      <td>Zerohedge.com</td>\n",
       "      <td>https://www.zerohedge.com/news/2018-04-23/msm-...</td>\n",
       "      <td>2018-04-23T23:55:00Z</td>\n",
       "      <td>MSM Is Frantically Attacking Dissenting Syria ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stephen Shankland</td>\n",
       "      <td>Even if you're happy SmugMug now runs Flickr, ...</td>\n",
       "      <td>Cnet.com</td>\n",
       "      <td>https://www.cnet.com/how-to/two-ways-to-get-yo...</td>\n",
       "      <td>2018-04-23T23:54:05Z</td>\n",
       "      <td>How to download all your Flickr photos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              author                                        description  \\\n",
       "0            pjbyrne  Disclaimer: English lawyer, not practising thi...   \n",
       "1       Tyler Durden  Authored by Caitlin Johnstone via Medium.com, ...   \n",
       "2  Stephen Shankland  Even if you're happy SmugMug now runs Flickr, ...   \n",
       "\n",
       "          publisher                                         source_url  \\\n",
       "0  Prestonbyrne.com  https://prestonbyrne.com/2018/04/23/on-ethereu...   \n",
       "1     Zerohedge.com  https://www.zerohedge.com/news/2018-04-23/msm-...   \n",
       "2          Cnet.com  https://www.cnet.com/how-to/two-ways-to-get-yo...   \n",
       "\n",
       "              timeStamp                                              title  \n",
       "0  2018-04-23T23:59:35Z                     Whether Ethereum is a security  \n",
       "1  2018-04-23T23:55:00Z  MSM Is Frantically Attacking Dissenting Syria ...  \n",
       "2  2018-04-23T23:54:05Z             How to download all your Flickr photos  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying correct data extraction\n",
    "print(\"\\nDataFrame Dimensions:\", newsDF.shape, \"\\n\")\n",
    "newsDF.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.3 Perform data cleanup and preprocessing.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions perform basic clean up on a dataframe. The purpose is to prepare the file to write-out (csv).  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'None' values\n",
    "def findNone(df):\n",
    "    \"\"\"\n",
    "     Receives pandas datraframe, and removes null entries from author feature\n",
    "    \"\"\"\n",
    "    print(\"Removed 'None' values in author feature...\")\n",
    "    author = df['author']\n",
    "    publisher = df['publisher']\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if pd.isnull(author.loc[i]):\n",
    "            author.loc[i] = publisher.loc[i]\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove gaps \n",
    "def gapStrip(df):\n",
    "    \"\"\"\n",
    "    Receives pandas dataframe and leading and traling empty space`\n",
    "    \"\"\"\n",
    "    df.columns = map(str.strip, df.columns) \n",
    "    print(\"Removed leading and trailing spaces and tabs...\")\n",
    "    # element-wise operation\n",
    "    f = lambda x: x.strip() if (isinstance(x,str)) else x\n",
    "    df = df.applymap(f)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize time stamps\n",
    "def std_timeStamp(df):\n",
    "    \"\"\"\n",
    "    Receives pandas dataframe and standardizes time stamps \n",
    "    \"\"\"\n",
    "    import datetime\n",
    "    # Check to see time stamps are in zero timezones\n",
    "    print(\"Converted Time Stamps to Desired Standard Formating...\")\n",
    "    for time in df['timeStamp']:\n",
    "        if time.endswith('Z'):\n",
    "            df['timeStamp'] = pd.to_datetime(df['timeStamp'],\n",
    "                                             infer_datetime_format = True,\n",
    "                                             utc = True)                       # returns a type '.Timestamp'\n",
    "            return(df)\n",
    "        else:\n",
    "            print(\"Revisit appropriate variable or function to deal with time zones that are not zero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_clean(df):\n",
    "    \"\"\"\n",
    "    Performs Generic Cleanup and Preprocessing on a given dataframe sourced from newsAPI\n",
    "    \"\"\"\n",
    "    \n",
    "    temp = findNone(df)           # removes missing values from author column\n",
    "    temp2 = gapStrip(temp)        # remove leading and trailing white space\n",
    "    temp3 = std_timeStamp(temp2)  # convert time stamps to 'utc' standard\n",
    "    return(temp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 'None' values in author feature...\n",
      "Removed leading and trailing spaces and tabs...\n",
      "Converted Time Stamps to Desired Standard Formating...\n"
     ]
    }
   ],
   "source": [
    "riskEx_df = feature_clean(newsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                          author  \\\n",
       " 0                        pjbyrne   \n",
       " 1                   Tyler Durden   \n",
       " 2              Stephen Shankland   \n",
       " 3  Ali Breland and Harper Neidig   \n",
       " 4               Cyberparse.co.uk   \n",
       " \n",
       "                                          description         publisher  \\\n",
       " 0  Disclaimer: English lawyer, not practising thi...  Prestonbyrne.com   \n",
       " 1  Authored by Caitlin Johnstone via Medium.com, ...     Zerohedge.com   \n",
       " 2  Even if you're happy SmugMug now runs Flickr, ...          Cnet.com   \n",
       " 3  THE EU'S NEW TARGET: They're back! European re...          The Hill   \n",
       " 4  Enlarge / Simplified figurative process of a C...  Cyberparse.co.uk   \n",
       " \n",
       "                                           source_url  \\\n",
       " 0  https://prestonbyrne.com/2018/04/23/on-ethereu...   \n",
       " 1  https://www.zerohedge.com/news/2018-04-23/msm-...   \n",
       " 2  https://www.cnet.com/how-to/two-ways-to-get-yo...   \n",
       " 3  http://thehill.com/policy/technology/overnight...   \n",
       " 4  http://cyberparse.co.uk/2018/04/23/new-hacks-s...   \n",
       " \n",
       "                   timeStamp                                              title  \n",
       " 0 2018-04-23 23:59:35+00:00                     Whether Ethereum is a security  \n",
       " 1 2018-04-23 23:55:00+00:00  MSM Is Frantically Attacking Dissenting Syria ...  \n",
       " 2 2018-04-23 23:54:05+00:00             How to download all your Flickr photos  \n",
       " 3 2018-04-23 23:49:29+00:00  Overnight Tech: EU investigates Apple's Shazam...  \n",
       " 4 2018-04-23 23:32:00+00:00  New hacks siphon private cryptocurrency keys f...  ,\n",
       "               author                                        description  \\\n",
       " 2491   Scott Scanlon  I see your Bitcoin shop and I raise you a Bitc...   \n",
       " 2492  Editorial Team  Digital banking startup Revolut has launched a...   \n",
       " 2493   Openstack.org  https://blueprints.launchpad.net/nova/+spec/pl...   \n",
       " 2494      Tony Sheng  Just as bitcoin is a promise for sound money 1...   \n",
       " 2495          157640  The Coinbase Webhooks API notifications servic...   \n",
       " \n",
       "                 publisher                                         source_url  \\\n",
       " 2491      Youbrandinc.com  https://www.youbrandinc.com/crytocurrency/redd...   \n",
       " 2492         Finextra.com  https://www.finextra.com/newsarticle/31953/rev...   \n",
       " 2493        Openstack.org  http://specs.openstack.org/openstack/nova-spec...   \n",
       " 2494        Tonysheng.com      https://www.tonysheng.com/sound-digital-goods   \n",
       " 2495  Programmableweb.com  https://www.programmableweb.com/api/coinbase-w...   \n",
       " \n",
       "                      timeStamp  \\\n",
       " 2491 2018-04-17 00:03:10+00:00   \n",
       " 2492 2018-04-17 00:01:00+00:00   \n",
       " 2493 2018-04-17 00:00:00+00:00   \n",
       " 2494 2018-04-17 00:00:00+00:00   \n",
       " 2495 2018-04-17 00:00:00+00:00   \n",
       " \n",
       "                                                   title  \n",
       " 2491  Reddit nerds are competing to prove their coun...  \n",
       " 2492         Revolut launches spare change savings tool  \n",
       " 2493                         Placement Forbidden Traits  \n",
       " 2494                                Sound digital goods  \n",
       " 2495                                  Coinbase Webhooks  )"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure data was preprocessed\n",
    "riskEx_df.head(5), riskEx_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7466 entries, 0 to 7465\n",
      "Data columns (total 6 columns):\n",
      "author         7466 non-null object\n",
      "description    7440 non-null object\n",
      "publisher      7466 non-null object\n",
      "source_url     7466 non-null object\n",
      "timeStamp      7466 non-null datetime64[ns, UTC]\n",
      "title          7463 non-null object\n",
      "dtypes: datetime64[ns, UTC](1), object(5)\n",
      "memory usage: 350.0+ KB\n"
     ]
    }
   ],
   "source": [
    "## Check file size\n",
    "riskEx_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.4 Write out to csv.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out n-csv-files each with 100 rows. Process is done to reduce computational load\n",
    "riskEx_df.to_csv('rawData5.csv', index_label = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2276"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('rawData5.csv')\n",
    "#print(df.info())\n",
    "print(len(df))\n",
    "len(df['description'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ if wanting to create batches of raw data files containing n-articles, use the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def df_to_csvs(df):\n",
    "#    articlesPage = int(100)\n",
    "#    totalArticles = len(df)\n",
    "#    batchSize=round(totalArticles/articlesPage)          # number of rows in single output file\n",
    "        \n",
    "#    for id, df_i in  enumerate(np.array_split(df, batchSize)):\n",
    "#        df_i.to_csv('rawData_{id}.csv'.format(id=id), index_label = False)                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **End Data Mining I:** Read-in NewsAPI feed for a given date range\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
