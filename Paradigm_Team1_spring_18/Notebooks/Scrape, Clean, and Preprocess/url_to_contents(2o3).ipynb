{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Packages and Libraries ##\n",
    "\n",
    "# Web parcing, scraping, etc.\n",
    "import bs4 as bs # BeautifulSoup4 \n",
    "import urllib3\n",
    "import re\n",
    "import requests # HTTP parser\n",
    "import html5lib\n",
    "\n",
    "# DataFrames and math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Output related packages \n",
    "import pprint as pp\n",
    "\n",
    "# read-in and write-out\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# stretch Jupyter coding blocks to fit screen\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\")) \n",
    "\n",
    "# make it run on py2 and py3\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining II\n",
    "This  notebook is intended to perform the following processes:\n",
    "\n",
    "    2.1 Read-in batched csv files and perform content extraction.\n",
    "\n",
    "    2.2 The notebook uses beautifulsoup to extract paragraph content of each url in batch.\n",
    "\n",
    "    2.3 Content extraction is written out as a matching csv file for future concatenation/merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### **Begin Data Mining II:** Per-url, article content extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.1 Read-in batched csv files and perform content extraction.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('rawData5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>description</th>\n",
       "      <th>publisher</th>\n",
       "      <th>source_url</th>\n",
       "      <th>timeStamp</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2493</th>\n",
       "      <td>Openstack.org</td>\n",
       "      <td>https://blueprints.launchpad.net/nova/+spec/pl...</td>\n",
       "      <td>Openstack.org</td>\n",
       "      <td>http://specs.openstack.org/openstack/nova-spec...</td>\n",
       "      <td>2018-04-17 00:00:00+00:00</td>\n",
       "      <td>Placement Forbidden Traits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2494</th>\n",
       "      <td>Tony Sheng</td>\n",
       "      <td>Just as bitcoin is a promise for sound money 1...</td>\n",
       "      <td>Tonysheng.com</td>\n",
       "      <td>https://www.tonysheng.com/sound-digital-goods</td>\n",
       "      <td>2018-04-17 00:00:00+00:00</td>\n",
       "      <td>Sound digital goods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>157640</td>\n",
       "      <td>The Coinbase Webhooks API notifications servic...</td>\n",
       "      <td>Programmableweb.com</td>\n",
       "      <td>https://www.programmableweb.com/api/coinbase-w...</td>\n",
       "      <td>2018-04-17 00:00:00+00:00</td>\n",
       "      <td>Coinbase Webhooks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author                                        description  \\\n",
       "2493  Openstack.org  https://blueprints.launchpad.net/nova/+spec/pl...   \n",
       "2494     Tony Sheng  Just as bitcoin is a promise for sound money 1...   \n",
       "2495         157640  The Coinbase Webhooks API notifications servic...   \n",
       "\n",
       "                publisher                                         source_url  \\\n",
       "2493        Openstack.org  http://specs.openstack.org/openstack/nova-spec...   \n",
       "2494        Tonysheng.com      https://www.tonysheng.com/sound-digital-goods   \n",
       "2495  Programmableweb.com  https://www.programmableweb.com/api/coinbase-w...   \n",
       "\n",
       "                      timeStamp                       title  \n",
       "2493  2018-04-17 00:00:00+00:00  Placement Forbidden Traits  \n",
       "2494  2018-04-17 00:00:00+00:00         Sound digital goods  \n",
       "2495  2018-04-17 00:00:00+00:00           Coinbase Webhooks  "
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.2 Use beautifulsoup to extract paragraph content of each url in batch.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__List of known path errors -- for validate_site -- is above the function call for batch_control__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_site(site, ignore):\n",
    "    \"\"\"\n",
    "    accepts single url, and a list. Iterates of list for paths to ignore.\n",
    "    If flag is found in list of know issues, url is flagged for removal.\n",
    "    Returns a flag -- '' -- for 'falsy' check \n",
    "    \"\"\"\n",
    "    from urllib.parse import urlparse\n",
    "    import copy\n",
    "    url = copy.copy(site) \n",
    "    \n",
    "    # Note: urlparse requires a string argument\n",
    "    info = urlparse(str(url))\n",
    "\n",
    "    for rm in ignore:\n",
    "        if rm in info.path:  # validates that site is not a podcast\n",
    "            return('')       # flagged for removal\n",
    "        else:\n",
    "            continue\n",
    "    return(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(site):\n",
    "    \"\"\"\n",
    "    Accepts dictionary of single key containing url. \n",
    "    Function uses beautiful soup to parse through paragraphs on given url\n",
    "    Returns the extracted data from paragraphs.\n",
    "    \"\"\"\n",
    "    \n",
    "    src = requests.get(site).content            # accesses content of html object\n",
    "    soup = bs.BeautifulSoup(src, 'lxml')        # object creation used in extracting paragraphs using built-in html parser\n",
    "    body = soup.find_all('p')                   # finds all paragraphs '<p>' in html object\n",
    "    return(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_content(body):\n",
    "    \"\"\"\n",
    "    Iterates over website content extracted by get_content \n",
    "    Function then splits the body into individual sentences and appends then to a list.\n",
    "    Returns said list\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence = [parags.text for parags in body]\n",
    "    text = \"\\t\".join(sentence)                  # tab delimeter for easier extraction\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_article(site):\n",
    "    \"\"\"\n",
    "    Accepts dict of 1 element containg url. \n",
    "    Function then uses other functions to validate the url -- checks for know issues and discards them.\n",
    "    Function, if no error is detected then calls function to extract article contents\n",
    "    Returns article contents\n",
    "    \"\"\"\n",
    "    \n",
    "    valid_site = validate_site(site, ignore)\n",
    "    if not valid_site:\n",
    "        return('Access Limit Met')                  # uses 'falsy' check if string is empty \n",
    "    else:\n",
    "        body = get_content(valid_site)\n",
    "        text = extract_from_content(body)\n",
    "        if not text:                                # uses 'falsy' check if string is empty \n",
    "            return('403 Forbidden')\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text(df):\n",
    "    \"\"\"\n",
    "    Accepts a 6xn dataframe and returns a 1xn list and a modifed dataframe\n",
    "    Note: stitching a dataframes to a list is much faster that updating existing one with a new column\n",
    "    \"\"\"\n",
    "    \n",
    "    import copy\n",
    "    content = []\n",
    "    url = []\n",
    "    author = copy.copy(df['author'])\n",
    "    link = copy.copy(df['source_url'])\n",
    "    \n",
    "    if author == 'Ml-implode.com': \n",
    "        r = requests.get(link)\n",
    "        soup = bs.BeautifulSoup(r.text, 'html.parser')\n",
    "        source = soup.find(id=\"LIJIT_title\")                   # manually found -- may differ moving forward\n",
    "        link = source.find('a').get('href')                    # gets the 'href inside the a tag -- i.e. the correct url\n",
    "        url.append(link)\n",
    "        content.append(get_article(link))                      # send correct url out for extraction\n",
    "    else:\n",
    "        content.append(get_article(link))\n",
    "        \n",
    "    return({'url':url, 'content':content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_df(df1, df2):\n",
    "    \"\"\"\n",
    "    Accepts a dataframe, and calls a function that checks for known aggregator, and known error sites. \n",
    "    If found, function replaces invalid entries with corrected ones, or drops them.\n",
    "    Returns corrected df.\n",
    "    \"\"\"\n",
    "    \n",
    "    import copy\n",
    "    author = copy.deepcopy(df1['author'])\n",
    "    df = copy.deepcopy(df1)\n",
    "    df['contents'] = \"\".join(df2['content']) # converts list to string\n",
    "    \n",
    "    #checks for know aggregator, replaces it with the url referenced by aggregator\n",
    "    if author == 'Ml-implode.com':\n",
    "        df['source_url'] = \"\".join(df2['url']) # converts 'list' item to 'str' item\n",
    "        return(df)        \n",
    "    else:\n",
    "        return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_author_name(df):\n",
    "    \"\"\"\n",
    "    Accepts dictionary of single url.\n",
    "    It then parses through url to extracts and returns netloc of url  \n",
    "    \"\"\"\n",
    "    \n",
    "    from urllib.parse import urlparse\n",
    "    import copy\n",
    "    url = copy.copy(df) \n",
    "    \n",
    "    # urlparse requires a string \n",
    "    info = urlparse(str(url))\n",
    "    return(info.netloc)          #.netloc extract the main url -- i.e. excludes path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'None' values\n",
    "def rm_false_author(df,start):\n",
    "    \"\"\"\n",
    "     Receives single row of pandas datraframe, and removes aggregator name from author feature -- if found\n",
    "     Returns original dataframe if no flag found\n",
    "    \"\"\"\n",
    "    \n",
    "    author = df['author']\n",
    "    source = df['source_url']\n",
    "    publisher = df['publisher']\n",
    "    \n",
    "    #checks for know aggregator, replaces it with the url referenced by aggregator\n",
    "    for i in range(len(df)):\n",
    "        if author.loc[i] == 'Ml-implode.com':\n",
    "            author.loc[start] = replace_author_name(source.loc[i])  # parameter is correct url as type 'str'\n",
    "            publisher.loc[start] = author.loc[i]                    # replaces instances of incorrect publisher name\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__List of known publishers and aggregators -- for stitch_df -- is a cell above the function call for batch_control__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_df(df, start, halt, init, discard):\n",
    "    \"\"\"\n",
    "    Accepts a dataframe, start/stop condition, as well as an initializer for proper indexing and a list of row-keys to drop\n",
    "    Controls all other associated functions related to data extraction\n",
    "    Return a preprocessed dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    import copy\n",
    "    df = copy.deepcopy(df)\n",
    "    data = pd.DataFrame()\n",
    "    start += init\n",
    "    halt += init\n",
    "    terminate = len(df) + init\n",
    "    author = df['author']\n",
    "    \n",
    "    # displays progress\n",
    "    if halt != terminate:\n",
    "        print(\"Batching range:\", start+1,\"-\", halt) \n",
    "    else:\n",
    "        print(\"Batching range:\", start+1,\"-\", terminate)\n",
    "        \n",
    "    # conditional merges dataframes and calls other functions that provide dataframe with data\n",
    "    while start < halt:\n",
    "        \n",
    "        ## print statement comes in handy for debugging ##\n",
    "        #print('\\n',author.loc[start], df['source_url'].loc[start])\n",
    "        \n",
    "        ### Checks for known bogus publisher or aggregator, disposes of row if error causing ###\n",
    "        for rm in discard:\n",
    "            while author.loc[start] == rm:\n",
    "                df.drop([start], axis = 0, inplace = True)\n",
    "                start += 1\n",
    "        ### -------------------------------------------------------------------------------- ###\n",
    "        \n",
    "        # datafram copy to prevent mutability \n",
    "        df1 = copy.deepcopy(df.loc[start])\n",
    "        df2 = get_text(df1)\n",
    "\n",
    "        datum = data.append(combine_df(df1,df2), ignore_index = True)  # append without dropping entries with same row number\n",
    "\n",
    "        ## minor clean up ##\n",
    "        # optional: data = data.drop(columns=['description'])          # removes redundant column        \n",
    "        data = rm_false_author(datum, start)                           # replaces invalid author entry\n",
    "\n",
    "        start += 1\n",
    "    return(data)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Batch Control and out-to-csv fail safe__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_control(df, batch, init, discard):\n",
    "    \"\"\"\n",
    "    Accepts a pandas df, and a batch size, and iterates through extraction and writting as batches.\n",
    "    The process is structured as such, to minimize computation time, and detect errors.\n",
    "    Does not have a Return. Used as master control.\n",
    "    \"\"\"\n",
    "    \n",
    "    start = 0\n",
    "    terminate = len(df)\n",
    "    print(\"Number of articles to be extracted:\",terminate)\n",
    "    print(\"Total number of batches:\", int(np.ceil(terminate/batch)),\"\\n\")\n",
    "    \n",
    "    while start < terminate:\n",
    "        \n",
    "        halt = start + batch   # Batch control\n",
    "    \n",
    "         # ensures we don't over expand range \n",
    "        if halt > terminate:        \n",
    "            halt = start + terminate\n",
    "            \n",
    "        temp = df[start:halt]\n",
    "        temp_out = stitch_df(temp, start, halt, init, discard)\n",
    "        \n",
    "        ### Creates/updates csv file to ensure process batches are not lost due to bugs or errors ###\n",
    "        try:\n",
    "            with open('riskEx_df5.csv') as file:\n",
    "                print('\\t Updating existing csv file')\n",
    "                temp_in = pd.read_csv('riskEx_df5.csv')\n",
    "                temp_out = temp_in.append(temp_out, ignore_index=True)\n",
    "                temp_out.to_csv('riskEx_df5.csv', index_label = False)\n",
    "                print('\\t Updated batch saved to csv')\n",
    "                pass\n",
    "        except IOError as e:\n",
    "            print(\"\\t Creating initial csv file\")\n",
    "            temp_out.to_csv('riskEx_df5.csv', index_label = False)\n",
    "            print('\\t Initial batch saved to csv')\n",
    "        ### ------------------------------------------------------------------------------------- ###\n",
    "        \n",
    "        start += batch         # Batch increment\n",
    "        \n",
    "    print(\"\\n*******************\\n PROJECT COMPLETED\\n*******************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.3 Write out content as csv file__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "---\n",
    "### __NOTE__ \n",
    "__In order to ensure there is zero data override, update the initializer to the same value as the begining of the dataframe slice prior to running batch_control.__ \n",
    "\n",
    "Failing to do so will at best throw an error, and at worst it will override your data. \n",
    "___\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 2496 articles in DataFrame.\n"
     ]
    }
   ],
   "source": [
    "##### KEY: initialize 'init' and the first row value in dataframe being passed #####\n",
    "#  Failiing to initialize 'project' and 'init' to same value may lead to data loss #\n",
    "\n",
    "print(\"\\nProcessing\",len(df),\"articles in DataFrame.\")\n",
    "batch_size = 100\n",
    "lhs = 2400\n",
    "rhs = 7000\n",
    "end = len(df)\n",
    "\n",
    "\n",
    "# Initialize project and init\n",
    "init = lhs \n",
    "project = df[lhs:]      # Approximate time for every 1000 articles:  ~25:00 \n",
    "##### ------------------------------------------------------------------------ #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Lists of common error causing paths and publishers__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of error causing paths\n",
    "ignore = ['mp3', 'rlslog', 'mo4ch', 'rex-silentium-bitcoin-suppressor', 'PD209.html',\n",
    "          'booking.com_considers_adding_flights_rules_out_bitcoin_acceptance', 'PD207.html',\n",
    "         'VL201.html', 'www.wikitimes.net', '1016879623', '.mp4', '1016330206', 'photos',\n",
    "         'General-Tech', 'energy', '5a78bbe5e4b00f94fe93fd8d', '4885392.htm', '.ece', '166075/',\n",
    "         'www.thetruthaboutcars.com', 'www.techishaky.com', 'www.anroed.com', 'dogecoin-wallet.html',\n",
    "         '4884023.htm', 'node/4249884', 'Visconti.html', 'a20180410PD210.html', 'whatthehack.life', \n",
    "         'ac7d222e4b07a3485e49e16', 'a20180330PD206.html', 'a20180326PD205.html', 'forty-seven',\n",
    "         'no-news-today', '1021452297']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Known bogus publishers, and aggregators. \n",
    "#List tells stitch_df to dispose of content where the author or url is one of the following\n",
    "\n",
    "discard = ['PA', 'https://holdernews.com', 'Blogspotpoint.com', 'Connor Madsen', \n",
    "           'Pressreleasepoint.com', 'Pewinternet.org', 'www.pewinternet.org', \n",
    "           'Jeremy Hellstrom', 'Ben Potter', 'Steph Willems', 'Thetruthaboutcars.com', \n",
    "           'Adam Tonge', 'lookout', 'noreply@blogger.com', 'Dan Tan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extract Content__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles to be extracted: 96\n",
      "Total number of batches: 1 \n",
      "\n",
      "Batching range: 2401 - 2496\n",
      "\t Updating existing csv file\n",
      "\t Updated batch saved to csv\n",
      "\n",
      "*******************\n",
      " PROJECT COMPLETED\n",
      "*******************\n"
     ]
    }
   ],
   "source": [
    "batch_control(project, batch_size, init, discard)  # execute batch extraction and write out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Finally:__ The following few cells carry out some minor preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2495 entries, 0 to 2494\n",
      "Data columns (total 7 columns):\n",
      "author         2482 non-null object\n",
      "contents       2495 non-null object\n",
      "description    2482 non-null object\n",
      "publisher      2495 non-null object\n",
      "source_url     2495 non-null object\n",
      "timeStamp      2495 non-null object\n",
      "title          2495 non-null object\n",
      "dtypes: object(7)\n",
      "memory usage: 155.9+ KB\n",
      "None\n",
      "2275\n"
     ]
    }
   ],
   "source": [
    "# Manual check \n",
    "riskEx = pd.read_csv('riskEx_df5.csv')\n",
    "print(riskEx.info())\n",
    "print(len(riskEx['description'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remove rows flagged for removal during preprossesing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2272\n"
     ]
    }
   ],
   "source": [
    "# drop duplicates, keep first instance of source-url\n",
    "riskEx.drop_duplicates(['description'], keep='first', inplace = True)\n",
    "print(len(riskEx['source_url'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of labels previously flagged for removal\n",
    "rm_403 = riskEx[riskEx['contents'] == '403 Forbidden'].index.values.tolist()\n",
    "rm_acc_denied = riskEx[riskEx['contents'] == 'Access Limit Met'].index.values.tolist()\n",
    "\n",
    "remove_rows = sorted(rm_403 + rm_forbidden + rm_acc_denied) # list of all values to be removed sorted for iteration\n",
    "print(\"Total unique rows after preprocessing:\", len(riskEx)-len(remove_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use remove_rows to remove observation from dataframe\n",
    "use_riskEx = riskEx.drop(remove_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2060 entries, 0 to 2494\n",
      "Data columns (total 7 columns):\n",
      "author         2048 non-null object\n",
      "contents       2060 non-null object\n",
      "description    2060 non-null object\n",
      "publisher      2060 non-null object\n",
      "source_url     2060 non-null object\n",
      "timeStamp      2060 non-null object\n",
      "title          2060 non-null object\n",
      "dtypes: object(7)\n",
      "memory usage: 128.8+ KB\n",
      "2057\n"
     ]
    }
   ],
   "source": [
    "use_riskEx.info()\n",
    "print(len(use_riskEx['source_url'].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Writting to disk__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN AFTER ENTIRE DF IS COMPLETELY EXTRACTED ##\n",
    "# saves dataframe as a preprocessed and cleaned (slightly) DataFrame\n",
    "use_riskEx.to_csv('use_riskEx5.csv', index_label = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__confirming everything was performed as expected__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2060 entries, 0 to 2494\n",
      "Data columns (total 7 columns):\n",
      "author         2048 non-null object\n",
      "contents       2060 non-null object\n",
      "description    2060 non-null object\n",
      "publisher      2060 non-null object\n",
      "source_url     2060 non-null object\n",
      "timeStamp      2060 non-null object\n",
      "title          2060 non-null object\n",
      "dtypes: object(7)\n",
      "memory usage: 128.8+ KB\n",
      "None\n",
      "2060\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv('use_riskEx5.csv')\n",
    "print(df2.info())\n",
    "print(len(df2['description'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2018-04-23 23:59:35+00:00\n",
      "1    2018-04-23 23:55:00+00:00\n",
      "3    2018-04-23 23:49:29+00:00\n",
      "Name: timeStamp, dtype: object\n",
      "2492    2018-04-17 00:00:00+00:00\n",
      "2493    2018-04-17 00:00:00+00:00\n",
      "2494    2018-04-17 00:00:00+00:00\n",
      "Name: timeStamp, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df2['timeStamp'].head(3))\n",
    "print(df2['timeStamp'].tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completed Process: \n",
    "##### riskEx_df dataframes are dataframes that contained extracted data but with no preprocessing performed.\n",
    "    They include more information, but there may be repeated values, or urls that are inaccessible\n",
    "    \n",
    "##### use_riskEx dataframes represent various dataframes of extracted features, their values, and the text content of each article's body.\n",
    "    The files have undergone preprocessing and duplicates have been removed. Also gone, unfortunatelly, are the descriptions of rows with urls that were not accessible using BS4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **End Data Mining II:** Per-url, article content extraction\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
